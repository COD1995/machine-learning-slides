\documentclass[9pt,dvipsnames]{beamer}
\usepackage[T1]{fontenc}
\usepackage{libertinus}
\usepackage{amsmath}
\usepackage[most]{tcolorbox}

\usepackage{hyperref}

\usepackage{xcolor}  
\newcommand{\cb}[1]{{\color{CadetBlue}#1}}
\setlength{\parskip}{0.3em}

\usetheme{Berkeley}
% \setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{}


\title{CSE574 Introduction to Machine Learning}
\subtitle{Continual Learning}
\author{Jue Guo}
\institute{University at Buffalo}
\date{\today}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Abstract}
\begin{frame}{Abstract}
    To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime.
    \begin{itemize}
        \item \textbf{continual learning}, a fundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by \textbf{catastrophic forgetting}, where learning a new task usually results in a dramatic performance degradation of the old tasks.
    \end{itemize}
\end{frame}

\section{Introduction}
\begin{frame}{Introduction}
    Learning is the basis for intelligent systems to accommodate environments. In response to external changes, evolution has empowered human and other organisms with strong adaptability to continually acquire, update, accumulate and exploit knowledge. Naturally we expect artifical intelligence (AI) systems to adapt in a similar way.
    \begin{itemize}
        \item This motivates the study of \textbf{continual learning}, where a typical setting is to learn a sequence of contents one by one and behave as if they were observed simultaneously. Such contents could be new skills, new examples of old skills, different environments, different contexts, etc., with particular realistic challenges incorporated.
        \item As the contents are provided incrementally over a lifetime, continual learnming is also referred to as \textbf{incremental learning} or \textbf{lifelong learning} in much of the literature, without a strict distinction.
    \end{itemize}
\end{frame}

\begin{frame}
    Unlike conventional machine learning models built on the premise of capturing a static data distribution, continual learning is characterized by learning from dynamic data distributions. A major challenge is known as \textbf{catastrophic forgetting}, where adaptation to a new distribution generally results in a largely reduced ability to capture old ones.
    \begin{itemize}
        \item This dilemma is a facet of the trade-off between \textbf{learning plasticity} and \textbf{memory stability}: an excess of the former interferes with the latter, and vice versa. A desirable solution for continual learning should obtain strong \textbf{generalizability} to accommodate distribution differences within and between tasks.
        \item A naive baseline, retraining all old training samples makes it easy to address the above challenges, but creates huge computational and storage overheads (as well as potential privacy issues).
    \end{itemize}
    In fact, continual learning is primarily intended to ensure the \textbf{resource efficiency} of model updates, perferably close to learning only new samples.
\end{frame}

\begin{frame}
    Numerous efforts have been devoted to addressing the above challenges, which can be conceptually seperated into five groups:
    \begin{enumerate}
        \item regularization-based approach
        \item replay-based approach
        \item optimization-based approach
        \item representation-based approach
        \item architecture-based approach
    \end{enumerate}
    These methods are \textit{closely connected}, e.g., regularization and replay ultimately act to rectify the gradient directions in optimization, and \textit{highly synergistic}, e.g., the efficacy of replay can be facilitated by distilling knowledge from the old model.
\end{frame}
\begin{frame}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\linewidth]{imgs/cl_1.png}
        \caption{A conceptual framework of continual learning. \textbf{a}, Continual learning requires adapting to incremental tasks with dynamic data distributions. \textbf{b}, A desirable solution should ensure a proper balance between stability (red arrow) and plasticity (green arrow), as well as an adequate generalizability to intra-task (blue arrow) and inter-task (orange arrow) distribution differences. \textbf{c}, Representative strategies have targeted various aspects of machine learning.}
    \end{figure}
\end{frame}
\begin{frame}
    Realistic applications present particular challenges for continual learning, which can be categorized into \textit{scenario complexity} and \textit{task specificity}.
    \begin{itemize}
        \item \textcolor{red}{As for the former}, for example, the task oracle (i.e., which task to perform) is probably missing in training and testing, and the training samples might be introduced in tiny batches or even one pass. Due to the expense and scarcity of data labeling, continual learning needs to be effective for few-shot, semi-supervised and even unsupervised scenarios.
        \item \textcolor{red}{As for the latter}, although current advances mainly focus on visual classification, other vision domains such as object detection, semantic segmentation and image generation, as well as other related fields, such as reinforcement learning (RL), natural language processing (NLP) and ethic considerations, are receiving increasing attention with their own opportunities and challenges.
    \end{itemize}
\end{frame}

\section{Setup}
\begin{frame}{Setup}
    \begin{itemize}
        \item Basic formulation of continual learning
        \item Typica scenarios
        \item Evaluation metrics
    \end{itemize}
\end{frame}

\subsection{Basic Formulation}
\begin{frame}{Basic Formulation}
    Continual learning is characterized as learning from dynamic data distributions. In practice, training samples of different distributions arrive in sequence. A continual learning model parameterized by $\theta$ needs to learn corresponding task(s) with no or limited access to old training samples and perform well on their test sets.

    \begin{itemize}
        \item Formally, an incoming batch of training samples belonging to a task $t$ can be represented as $\mathcal{D}_{t, b}=\left\{\mathcal{X}_{t, b}, \mathcal{Y}_{t, b}\right\}$, where $\mathcal{X}_{t, b}$ is the input data, $\mathcal{Y}_{t, b}$ is the data label, $t \in \mathcal{T}=\{1, \cdots, k\}$ is the task identity and $b \in \mathcal{B}_{t}$ is the batch index $\left(\mathcal{T}\right.$ and $\mathcal{B}_{t}$ denote their space, respectively).
        \item Here we define a "task" by its training samples $\mathcal{D}_{t}$ following the distribution $\mathbb{D}_{t}:=p\left(\mathcal{X}_{t}, \mathcal{Y}_{t}\right)\left(\mathcal{D}_{t}\right.$ denotes the entire training set by omitting the batch index, likewise for $\mathcal{X}_{t}$ and $\mathcal{Y}_{t}$ ), and assume that there is no difference in distribution between training and testing. Under realistic constraints, the data label $\mathcal{Y}_{t}$ and the task identity $t$ might not be always available.
    \end{itemize}
    In continual learning, the training samples of each task can arrive incrementally in batches (i.e., $\left\{\left\{\mathcal{D}_{t, b}\right\}_{b \in \mathcal{B}_{t}}\right\}_{t \in \mathcal{T}}$ ) or simultaneously (i.e., $\left\{\mathcal{D}_{t}\right\}_{t \in \mathcal{T}}$ ).
\end{frame}

\subsection{Typical Scenario}
\begin{frame}{Typical Scenario}
    According to the division of incremental batches and the availability of task identities, we detail the typical continual learning scenarios as follows:
    \begin{itemize}
        \item \textit{Instance-Incremental Learning} (IIL): All training samples belong to the same task and arrive in batches.
        \item \textit{Domain-Incremental Learning} (DIL): Tasks have the same data label space but different input distributions. Task identities are not required.
        \item \textit{Task-Incremental Learning} (TIL): Tasks have disjoint data label spaces. Task identities are provided in both training and testing.
        \item \textit{Class-Incremental Learning} (CIL): Tasks have disjoint data label spaces. Task identities are only provided in training.
        \item \textit{Task-Free Continual Learning} (TFCL): Tasks have disjoint data label spaces. Task identities are not provided in either training or testing.
        \item \textit{Online Continual Learning} (OCL): Tasks have disjoint data label spaces. Training samples for each task arrive as a one-pass data stream.
        \item \textit{Blurred Boundary Continual Learning} (BBCL): Task boundaries are blurred, characterized by distinct but overlapping data label spaces.
        \item \textit{Continual Pre-training} (CPT): Pre-training data arrives in sequence. The goal is to improve the performance of learning downstream tasks.
    \end{itemize}
\end{frame}

\begin{frame}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=\linewidth]{imgs/cl_2.png}
        \caption{A formal comparison of typical continual learning scenarios. $\mathcal{D}_{t, b}$ : the training samples of task $t$ and batch $b .|b|$ : the size of batch $b . \mathcal{B}_{t}$ : the space of incremental batches belonging to task $t \cdot \mathcal{D}_{t}$ : the training set of task $t$ (further specified as $\mathcal{D}_{t}^{p t}$ for pre-training). $\mathcal{T}$ : the space of all incremental tasks (further specified as $\mathcal{T}^{p t}$ for pre-training). $\mathcal{X}_{t}$ : the input data in $\mathcal{D}_{t} . p\left(\mathcal{X}_{t}\right)$ : the distribution of $\mathcal{X}_{t} . \mathcal{Y}_{t}$ : the data label of $\mathcal{X}_{t}$.}
    \end{figure}
\end{frame}
\begin{frame}
    If not specified, each task is generally assumed to have a sufficient number of labeled training samples, i.e., Supervised Continual Learning.
    \begin{itemize}
        \item According to the provided $\mathcal{X}_{t}$ and $\mathcal{Y}_{t}$ in each $\mathcal{D}_{t}$, continual learning can be further categorized into zero-shot, few-shot, semi-supervised, open-world (i.e., to identify unknown classes and then incorporate their labels)  and un-/self-supervised scenarios.
    \end{itemize}
    Besides, other practical challenges have been considered and incorporated,
    \begin{itemize}
        \item such as multiple labels , noisy labels, hierarchical granularity and sub-populations , mixture of task similarity , long-tailed distribution , domain alignment , domain shifting, anytime inference, novel class discovery, multi-modality, etc.
    \end{itemize}
    Some recent work has focused on various combinations of these scenarios, as a way of better simulating the complexity of the real world.
\end{frame}

\subsection{Evaluation Metric}
\begin{frame}{Evaluation Metric}
    In general, the performance of continual learning can be evaluated from three aspects: overall performance of the tasks learned so far, memory stability of old tasks, and learning plasticity of new tasks.
    \begin{itemize}
        \item Here we summarize the common evaluation metrics, using classification as an example. \textbf{Overall performance, Memory stability}, and \textbf{Learning plasticity}.
    \end{itemize}
\end{frame}

\begin{frame}{Overall Performance}
    \textbf{Overall performance} is typically evaluated by \textit{average accuracy} (AA) and \textit{average incremental accuracy} (AIA).
    \begin{itemize}
        \item Let $a_{k, j} \in[0,1]$ denote the classification accuracy evaluated on the test set of the $j$-th task after incremental learning of the $k$-th task $(j \leq k)$. The output space to compute $a_{k, j}$ consists of the classes in either $\mathcal{Y}_{j}$ or $\cup_{i=1}^{k} \mathcal{Y}_{i}$, corresponding to the use of multi-head evaluation (e.g., TIL) or single-head evaluation (e.g., CIL).
    \end{itemize}
    The two metrics at the $k$-th task are then defined as
    $$
        \mathrm{AA}_{k}=\frac{1}{k} \sum_{j=1}^{k} a_{k, j}
    $$
    $$
        \mathrm{AIA}_{k}=\frac{1}{k} \sum_{i=1}^{k} \mathrm{AA}_{i}
    $$
    where AA represents the overall performance at the current moment and AIA further reflects the historical variation.
\end{frame}

\begin{frame}{Memory Stability}
    \textbf{Memory stability} can be evaluated by \textit{forgetting measure} (FM) and \textit{backward transfer} (BWT). As for the former, the forgetting of a task is calculated by the difference between its maximum performance obtained in the past and its current performance:

    $$
        f_{j, k}=\max _{i \in\{1, \ldots, k-1\}}\left(a_{i, j}-a_{k, j}\right), \forall j<k .
    $$

    FM at the $k$-th task is the average forgetting of all old tasks:

    $$
        \mathrm{FM}_{k}=\frac{1}{k-1} \sum_{j=1}^{k-1} f_{j, k}
    $$

    As for the latter, BWT evaluates the average influence of learning the $k$-th task on all old tasks:

    $$
        \mathrm{BWT}_{k}=\frac{1}{k-1} \sum_{j=1}^{k-1}\left(a_{k, j}-a_{j, j}\right)
    $$

    where the forgetting is usually reflected as a negative BWT.
\end{frame}

\begin{frame}{Learning Plasticity}
    \textbf{Learning plasticity} can be evaluated by \textit{intransience measure} (IM) and \textit{forward transfer} (FWT). IM is defined as the inability of a model to learn new tasks, calculated by the difference of a task between its joint training performance and continual learning performance:

    $$
        \mathrm{IM}_{k}=a_{k}^{*}-a_{k, k}
    $$

    where $a_{k}^{*}$ is the classification accuracy of a randomlyinitialized reference model jointly trained with $\cup_{j=1}^{k} \mathcal{D}_{j}$ for the $k$-th task.

    In comparison, FWT evaluates the average influence of all old tasks on the current $k$-th task:

    $$
        \mathrm{FWT}_{k}=\frac{1}{k-1} \sum_{j=2}^{k}\left(a_{j, j}-\tilde{a}_{j}\right)
    $$

    where $\tilde{a}_{j}$ is the classification accuracy of a randomlyinitialized reference model trained with $\mathcal{D}_{j}$ for the $j$-th task. Note that, $a_{k, j}$ can be adapted to other forms depending on the task type, such as average precision (AP) for object detection, Intersection-over-Union (IoU) for semantic segmentation, FrÃ©chet Inception Distance (FID) for image generation, normalized reward for reinforcement learning, etc, and the above evaluation metrics should be adjusted accordingly.
\end{frame}
\section{Theoretical Foundation and Analysis}
\begin{frame}{Theoretical Foundation and Analysis}
    Theoretical efforts in continual learning, with respect to stability-plasticity trade-off and generalizability analysis, and relate them to the main motivation for representative strategies.
\end{frame}

\subsection{Stability-Plasticity Trade-off}
\begin{frame}{Stability-Plasticity Trade-off}
    Let's consider a general setup for continual learning, where a neural network with parameters $\theta \in \mathbb{R}^{|\theta|}$ needs to learn $k$ incremental tasks.
    \begin{itemize}
        \item The training set and test set of each task are assumed to follow the same distribution $\mathbb{D}_{t}, t=1, \ldots, k$, where the training set $\mathcal{D}_{t}=\left\{\mathcal{X}_{t}, \mathcal{Y}_{t}\right\}=\left\{\left(x_{t, n}, y_{t, n}\right)\right\}_{n=1}^{N_{t}}$ includes $N_{t}$ data-label pairs.
    \end{itemize}
    The objective is to learn a probabilistic model $p\left(\mathcal{D}_{1: k} \mid \theta\right)=\prod_{t=1}^{k} p\left(\mathcal{D}_{t} \mid \theta\right)$ (with assumption of conditional independence) that can perform well on all tasks denoted as $\mathcal{D}_{1: k}:=\left\{\mathcal{D}_{1}, \ldots, \mathcal{D}_{k}\right\}$.
    \begin{itemize}
        \item The task-related performance for discriminative models can be expressed as $\log p\left(\mathcal{D}_{t} \mid \theta\right)=\sum_{n=1}^{N_{t}} \log p_{\theta}\left(y_{t, n} \mid x_{t, n}\right)$.
    \end{itemize}
    The central challenge of continual learning generally arises from the sequential nature of learning: when learning the $k$-th task from $\mathcal{D}_{k}$, the old training sets $\left\{\mathcal{D}_{1}, \ldots, \mathcal{D}_{k-1}\right\}$ are inaccessible. Therefore, it is critical but extremely challenging to capture the distributions of both old and new tasks in a balanced manner, i.e., ensuring an appropriate \textbf{stability-plasticity trade-off}, where excessive learning plasticity or memory stability can largely compromise each other.
\end{frame}

\begin{frame}{}
    A straightforward idea is to approximate and recover the old data distributions by storing a few old training samples or training a generative model, known as the replay-based approach.
    \begin{itemize}
        \item According to the learning theory for supervised learning, the performance of an old task is improved with replaying more old training samples that approximate its distribution, but with potential privacy issues and a linear increase in \textbf{resource overhead}.
        \item The use of generative models is also limited by the huge resource overhead, as well as their own catastrophic forgetting and expressiveness.
    \end{itemize}
\end{frame}

\begin{frame}
    An alternative choice is to propagate the old data distributions in updating parameters through formulating continual learning in a \textit{Bayesian framework}.
    \begin{itemize}
        \item Based on a prior, \(p(\theta)\) the posterior after observing the \(k\)-th task can be computed with Bayes' rule:
              $$
                  p\left(\theta \mid \mathcal{D}_{1: k}\right) \propto p(\theta) \prod_{t=1}^{k} p\left(\mathcal{D}_{t} \mid \theta\right) \propto p\left(\theta \mid \mathcal{D}_{1: k-1}\right) p\left(\mathcal{D}_{k} \mid \theta\right)
              $$
              where the posterior $p\left(\theta \mid \mathcal{D}_{1: k-1}\right)$ of the $(k-1)$-th task becomes the prior of the $k$-th task, and thus enables the new posterior $p\left(\theta \mid \mathcal{D}_{1: k}\right)$ to be computed with only the current training set $\mathcal{D}_{k}$.
    \end{itemize}
    However, as the posterior is generally intractable (except very special cases), a common option is to approximate it with $q_{k-1}(\theta) \approx p\left(\theta \mid \mathcal{D}_{1: k-1}\right)$, likewise for $q_{k}(\theta) \approx p\left(\theta \mid \mathcal{D}_{1: k}\right)$.
\end{frame}

\begin{frame}{Laplace Approximation}
    In the following, we will introduce two widely-used approximation strategies:

    The first is \textit{online Laplace approximation}, which approximates $p\left(\theta \mid \mathcal{D}_{1: k-1}\right)$ as a multivariate Gaussian with local gradient information.
    \begin{itemize}
        \item Specifically, we can parameterize $q_{k-1}(\theta)$ with $\phi_{k-1}$ and construct an approximate Gaussian posterior $q_{k-1}(\theta):=q\left(\theta ; \phi_{k-1}\right)=$ $\mathcal{N}\left(\theta ; \mu_{k-1}, \Lambda_{k-1}^{-1}\right)$ through performing a second-order Taylor expansion around the mode $\mu_{k-1} \in \mathbb{R}^{|\theta|}$ of $p\left(\theta \mid \mathcal{D}_{1: k-1}\right)$, where $\Lambda_{k-1}$ denotes the precision matrix and $\phi_{k-1}=$ $\left\{\mu_{k-1}, \Lambda_{k-1}\right\}$, likewise for $q\left(\theta ; \phi_{k}\right), \mu_{k}$ and $\Lambda_{k}$.
    \end{itemize}
    The posterior mode for learning the current $k$-th task can be computed as

    $$
        \begin{aligned}
            \mu_{k} & =\arg \max _{\theta} \log p\left(\theta \mid \mathcal{D}_{1: k}\right)                                                                                          \\
                    & \approx \arg \max _{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right)+\log q\left(\theta ; \phi_{k-1}\right)                                               \\
                    & =\arg \max _{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right)-\frac{1}{2}\left(\theta-\mu_{k-1}\right)^{\top} \Lambda_{k-1}\left(\theta-\mu_{k-1}\right),
        \end{aligned}
    $$
    which can be obtained from $\mu_{k-1}$ and $\Lambda_{k-1}$.
\end{frame}

\begin{frame}
    Similarly, $\Lambda_{k}$ can be updated recursively:

    $$
        \begin{aligned}
            \Lambda_{k} & =-\left.\nabla_{\theta}^{2} \log p\left(\theta \mid \mathcal{D}_{1: k}\right)\right|_{\theta=\mu_{k}}                  \\
                        & \approx-\left.\nabla_{\theta}^{2} \log p\left(\mathcal{D}_{k} \mid \theta\right)\right|_{\theta=\mu_{k}}+\Lambda_{k-1}
        \end{aligned}
    $$

    where the first term on the right side is the Hessian of the negative $\log$ likelihood of $\mathcal{D}_{k}$ at $\mu_{k}$, denoted as $H\left(\mathcal{D}_{k}, \mu_{k}\right)$.
    \begin{itemize}
        \item In practice, $H\left(\mathcal{D}_{k}, \mu_{k}\right)$ is often computationally inefficient due to the great dimensionality of $\mathbb{R}^{|\theta|}$, and there is no guarantee that the approximated $\Lambda_{k}$ is positive semi-definite for the Gaussian assumption.
    \end{itemize}
    To overcome these issues, the Hessian can be approximated by the Fisher information matrix (FIM):

    $$F_{k}=\left.\mathbb{E}\left[\nabla_{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right) \nabla_{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right)^{\top}\right]\right|_{\theta=\mu_{k}} \approx H\left(\mathcal{D}_{k}, \mu_{k}\right)$$

    The computation of FIM can be further simplified with a diagonal approximation or a Kronecker-factored approximation.
\end{frame}

\begin{frame}
    Formally,
    $$
        \begin{aligned}
            \mu_{k} & =\arg \max _{\theta} \log p\left(\theta \mid \mathcal{D}_{1: k}\right)                                                                                          \\
                    & \approx \arg \max _{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right)+\log q\left(\theta ; \phi_{k-1}\right)                                               \\
                    & =\arg \max _{\theta} \log p\left(\mathcal{D}_{k} \mid \theta\right)-\frac{1}{2}\left(\theta-\mu_{k-1}\right)^{\top} \Lambda_{k-1}\left(\theta-\mu_{k-1}\right),
        \end{aligned}
    $$
    is implemented by saving a copy of the old model $\mu_{k-1}$ to regularize parameter changes, which is known as the \textit{regularization based approach}. Here, we use EWC as an example and present its loss function:
    $$
        \mathcal{L}_{\mathrm{EWC}}(\theta)=\ell_{k}(\theta)+\frac{\lambda}{2}\left(\theta-\mu_{k-1}\right)^{\top} \hat{F}_{1: k-1}\left(\theta-\mu_{k-1}\right),
    $$
    where $\ell_{k}$ denotes the task-specific loss, the FIM $\hat{F}_{1: k-1}=$ $\sum_{t=1}^{k-1} \operatorname{diag}\left(F_{t}\right)$ with a diagonal approximation $\operatorname{diag}(\cdot)$ of each $F_{t}$, and $\lambda$ is a hyperparameter to control the strength of regularization.
\end{frame}

\begin{frame}{Variational Inference}
    The second is \textit{online variational inference} (VI).
    \begin{itemize}
        \item There are many different ways to do this, and a representative one is to minimize the following KL-divergence over a family $\mathcal{Q}$ that satisfies $p\left(\theta \mid \mathcal{D}_{1: k}\right) \in \mathcal{Q}$ at the current $k$-th task:
              $$
                  q_{k}(\theta)=\arg \min _{q \in \mathcal{Q}} \operatorname{KL}\left(q(\theta) \| \frac{1}{Z_{k}} q_{k-1}(\theta) p\left(\mathcal{D}_{k} \mid \theta\right)\right),
              $$
              where $Z_{k}$ is the normalizing constant of $q_{k-1}(\theta) p\left(\mathcal{D}_{k} \mid \theta\right)$.
    \end{itemize}
    In practice, the above minimization can be achieved by employing an additional Monte Carlo approximation, with specifying $q_{k}(\theta):=q\left(\theta ; \phi_{k}\right)=\mathcal{N}\left(\theta ; \mu_{k}, \Lambda_{k}^{-1}\right)$ as a multivariate Gaussian.

    Here we use VCL as an example, which minimizes the following objective (i.e., maximize its  negative):
    $$
        \mathcal{L}_{\mathrm{VCL}}\left(q_{k}(\theta)\right)=\mathbb{E}_{q_{k}(\theta)}\left(\ell_{k}(\theta)\right)+\mathrm{KL}\left(q_{k}(\theta) \| q_{k-1}(\theta)\right),
    $$
    where the KL-divergence can be computed in a closed-form  and serves as an implicit regularization term.

\end{frame}

\begin{frame}
    The loss function we have seen from the two approaches,
    $$
        \mathcal{L}_{\mathrm{EWC}}(\theta)=\ell_{k}(\theta)+\frac{\lambda}{2}\left(\theta-\mu_{k-1}\right)^{\top} \hat{F}_{1: k-1}\left(\theta-\mu_{k-1}\right),
    $$
    and,
    $$
        \mathcal{L}_{\mathrm{VCL}}\left(q_{k}(\theta)\right)=\mathbb{E}_{q_{k}(\theta)}\left(\ell_{k}(\theta)\right)+\mathrm{KL}\left(q_{k}(\theta) \| q_{k-1}(\theta)\right),
    $$
    take similar forms, the former is a local approximation at a set of deterministic parameters $\theta$, while the latter is computed by sampling from the variational distribution $q_{k}(\theta)$.

    This is  attributed to the fundamental difference between the two approximation strategies with slightly different performance in particular settings.
\end{frame}

\begin{frame}
    In addition to the parameter space, the idea of sequential Bayesian inference is also applicable to the function space, which tends to enable more flexibility in updating parameters. Also, there are many other extensions of VI, such as improving posterior updates with variational auto-regressive Gaussian processes (VAR-GPs), constructing task-specific parameters, and adapting to a non-stationary data stream.
\end{frame}
\end{document}